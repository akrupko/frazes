#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ table_phrases.json."""

from __future__ import annotations

import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple


def normalize_phrase(phrase: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ—Ä–∞–∑—É: —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É."""
    return " ".join(phrase.lower().strip().split())


def merge_duplicate_entries(entries: List[Dict]) -> Dict:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–µ–π –≤ –æ–¥–Ω—É, —Å–æ–±–∏—Ä–∞—è –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."""
    base = entries[0]
    merged = {k: v for k, v in base.items() if k not in {"phrase", "meanings", "etymology", "source_url", "category"}}
    merged["phrase"] = " ".join(base.get("phrase", "").strip().split())

    # Meanings
    merged_meanings: List[str] = []
    seen_meanings = set()
    for entry in entries:
        for meaning in entry.get("meanings", []) or []:
            normalized = meaning.strip()
            if normalized and normalized not in seen_meanings:
                seen_meanings.add(normalized)
                merged_meanings.append(normalized)
    merged["meanings"] = merged_meanings

    # Etymology
    merged["etymology"] = ""
    for entry in entries:
        etym = (entry.get("etymology") or "").strip()
        if etym:
            merged["etymology"] = etym
            break

    # Categories
    categories: List[str] = []
    seen_categories = set()
    for entry in entries:
        category = (entry.get("category") or "").strip()
        if category and category not in seen_categories:
            seen_categories.add(category)
            categories.append(category)
    if len(categories) > 1:
        merged["category"] = ", ".join(categories)
    elif categories:
        merged["category"] = categories[0]
    else:
        merged["category"] = ""

    # Source URLs
    source_urls: List[str] = []
    seen_urls = set()
    for entry in entries:
        url = (entry.get("source_url") or "").strip()
        if url and url not in seen_urls:
            seen_urls.add(url)
            source_urls.append(url)
    if len(source_urls) > 1:
        merged["source_url"] = " | ".join(source_urls)
    elif source_urls:
        merged["source_url"] = source_urls[0]
    else:
        merged["source_url"] = ""

    return merged


def group_phrases(phrases: List[Dict]) -> Tuple[Dict[str, List[Dict]], List[str]]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ—Ä–∞–∑—ã –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É –∫–ª—é—á—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø–æ—è–≤–ª–µ–Ω–∏—è."""
    groups: Dict[str, List[Dict]] = {}
    order: List[str] = []

    for entry in phrases:
        normalized = normalize_phrase(entry.get("phrase", ""))
        if normalized not in groups:
            groups[normalized] = []
            order.append(normalized)
        groups[normalized].append(entry)

    return groups, order


def find_and_remove_duplicates(input_path: Path, output_path: Path) -> Dict:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª, –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –æ—á–∏—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é."""
    print("=" * 80)
    print("–ê–ù–ê–õ–ò–ó –ò –û–ß–ò–°–¢–ö–ê –î–£–ë–õ–ò–ö–ê–¢–û–í –í table_phrases.json")
    print("=" * 80)
    print()

    with input_path.open("r", encoding="utf-8") as source_file:
        data = json.load(source_file)

    phrases = data.get("phrases", [])
    total_before = len(phrases)
    print(f"üìä –í—Å–µ–≥–æ —Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º–æ–≤ –¥–æ –æ—á–∏—Å—Ç–∫–∏: {total_before}")
    print()

    phrase_groups, order = group_phrases(phrases)

    duplicates: Dict[str, List[Dict]] = {}
    duplicates_order: List[str] = []
    for normalized in order:
        entries = phrase_groups[normalized]
        if len(entries) > 1:
            duplicates[normalized] = entries
            duplicates_order.append(normalized)

    num_duplicates = len(duplicates)
    total_duplicate_entries = sum(len(entries) - 1 for entries in duplicates.values())
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏: {num_duplicates}")
    print(f"üîç –í—Å–µ–≥–æ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π: {total_duplicate_entries}")
    print()

    if duplicates:
        print("=" * 80)
        print("–°–ü–ò–°–û–ö –î–£–ë–õ–ò–ö–ê–¢–û–í")
        print("=" * 80)
        print()
        for idx, normalized in enumerate(duplicates_order, 1):
            entries = duplicates[normalized]
            merged = merge_duplicate_entries(entries)
            print(f"{idx}. –§—Ä–∞–∑–∞: '{merged['phrase']}'")
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π: {len(entries)}")
            print("   –í–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø–∏—Å–µ–π:")
            for variant_idx, entry in enumerate(entries, 1):
                print(f"      {variant_idx}) '{entry.get('phrase', '')}'")
                print(f"         Meanings: {entry.get('meanings', [])}")
                print(f"         Category: {entry.get('category', '')}")
                print(f"         Etymology: {'–ï—Å—Ç—å' if entry.get('etymology') else '–ù–µ—Ç'}")
                print(f"         Source URL: {entry.get('source_url', '')}")
            print("   –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
            print(f"      Meanings ({len(merged['meanings'])}): {merged['meanings']}")
            print(f"      Category: {merged.get('category', '')}")
            print(f"      Source URL: {merged.get('source_url', '')}")
            print(f"      Etymology: {'–ï—Å—Ç—å' if merged.get('etymology') else '–ù–µ—Ç'}")
            print()

    cleaned_phrases: List[Dict] = []
    for normalized in order:
        cleaned_phrases.append(merge_duplicate_entries(phrase_groups[normalized]))

    total_after = len(cleaned_phrases)
    with output_path.open("w", encoding="utf-8") as cleaned_file:
        json.dump({"phrases": cleaned_phrases}, cleaned_file, ensure_ascii=False, indent=2)

    print("=" * 80)
    print("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢")
    print("=" * 80)
    print(f"üìä –í—Å–µ–≥–æ —Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º–æ–≤ –¥–æ –æ—á–∏—Å—Ç–∫–∏: {total_before}")
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏: {num_duplicates}")
    print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π: {total_duplicate_entries}")
    print(f"‚úÖ –§—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {total_after}")
    print(f"üìù –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
    print()

    return {
        "total_before": total_before,
        "duplicates_found": num_duplicates,
        "duplicate_entries_removed": total_duplicate_entries,
        "total_after": total_after,
        "duplicates_detail": duplicates,
        "duplicates_order": duplicates_order,
    }


def generate_markdown_report(
    results: Dict,
    report_path: Path,
    *,
    main_file: Path,
    cleaned_file: Path,
    backup_file: Path,
) -> None:
    """–°–æ–∑–¥–∞–µ—Ç –æ—Ç—á—ë—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∏ —Å–ø–∏—Å–∫–æ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤."""
    timestamp = datetime.now().strftime("%d.%m.%Y %H:%M:%S")
    lines: List[str] = [
        "# –û—Ç—á—ë—Ç –æ–± –æ—á–∏—Å—Ç–∫–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ table_phrases.json",
        "",
        f"**–î–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {timestamp}",
        "**–°–∫—Ä–∏–ø—Ç:** `deduplicate_phrases.py`",
        "",
        "---",
        "",
        "## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
        "",
        "| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ |",
        "|----------|----------|",
        f"| **–í—Å–µ–≥–æ —Ñ—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º–æ–≤ –¥–æ –æ—á–∏—Å—Ç–∫–∏** | {results['total_before']} |",
        f"| **–ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑ —Å –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏** | {results['duplicates_found']} |",
        f"| **–í—Å–µ–≥–æ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π —É–¥–∞–ª–µ–Ω–æ** | {results['duplicate_entries_removed']} |",
        f"| **–§—Ä–∞–∑–µ–æ–ª–æ–≥–∏–∑–º–æ–≤ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏** | {results['total_after']} |",
        "",
        "---",
        "",
        "## üîç –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã",
        "",
    ]

    duplicates_detail: Dict[str, List[Dict]] = results["duplicates_detail"]
    duplicates_order: List[str] = results["duplicates_order"]

    if not duplicates_detail:
        lines.append("–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã.")
        lines.append("")
    else:
        for idx, normalized in enumerate(duplicates_order, 1):
            entries = duplicates_detail[normalized]
            merged = merge_duplicate_entries(entries)
            lines.append(f"### {idx}. –§—Ä–∞–∑–∞: \"{merged['phrase']}\"")
            lines.append("")
            lines.append(f"**–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π:** {len(entries)}")
            lines.append("")
            lines.append("#### –ò—Å—Ö–æ–¥–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∑–∞–ø–∏—Å–µ–π:")
            lines.append("")
            for variant_idx, entry in enumerate(entries, 1):
                lines.append(f"**–í–∞—Ä–∏–∞–Ω—Ç {variant_idx}:**")
                lines.append(f"- **Phrase:** {entry.get('phrase', '')}")
                meanings = entry.get("meanings", []) or []
                if meanings:
                    lines.append("- **Meanings:**")
                    for meaning in meanings:
                        lines.append(f"  - {meaning}")
                else:
                    lines.append("- **Meanings:** _–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_")
                category = entry.get("category") or ""
                lines.append(f"- **Category:** {category if category else '_–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_'}")
                etymology = entry.get("etymology") or ""
                lines.append(f"- **Etymology:** {'–ï—Å—Ç—å' if etymology.strip() else '–ù–µ—Ç'}")
                source = entry.get("source_url") or ""
                lines.append(f"- **Source URL:** {source if source else '_–Ω–µ —É–∫–∞–∑–∞–Ω_'}")
                lines.append("")

            lines.append("#### –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:")
            lines.append("")
            merged_meanings = merged.get("meanings", [])
            if merged_meanings:
                lines.append("- **Meanings:**")
                for meaning in merged_meanings:
                    lines.append(f"  - {meaning}")
            else:
                lines.append("- **Meanings:** _–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_")
            merged_category = merged.get("category") or ""
            lines.append(f"- **Category:** {merged_category if merged_category else '_–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_'}")
            merged_etymology = merged.get("etymology") or ""
            if merged_etymology:
                lines.append(f"- **Etymology:** {merged_etymology}")
            else:
                lines.append("- **Etymology:** _–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_")
            merged_source = merged.get("source_url") or ""
            lines.append(f"- **Source URL:** {merged_source if merged_source else '_–Ω–µ —É–∫–∞–∑–∞–Ω_'}")
            lines.append("")
            lines.append("---")
            lines.append("")

    lines.extend(
        [
            "## üìÅ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã",
            "",
            f"1. **{main_file.name}** ‚Äî –æ—á–∏—â–µ–Ω–Ω—ã–π –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª",
            f"2. **{cleaned_file.name}** ‚Äî —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞",
            f"3. **{backup_file.name}** ‚Äî —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
            "4. **DEDUPLICATION_REPORT.md** ‚Äî –¥–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç",
            "",
            "---",
            "",
            "## üîÑ –ü–æ–≤—Ç–æ—Ä–Ω—ã–π –∑–∞–ø—É—Å–∫",
            "",
            "–ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ backup –∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–∫—Ä–∏–ø—Ç:",
            "",
            "```bash",
            f"cp {backup_file.name} {main_file.name}",
            "python3 deduplicate_phrases.py",
            "```",
            "",
            "–°–∫—Ä–∏–ø—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—Å—Ç –Ω–æ–≤—É—é —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é, –Ω–∞–π–¥–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–±–Ω–æ–≤–∏—Ç –æ—Ç—á—ë—Ç.",
        ]
    )

    report_path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def main() -> None:
    input_path = Path("table_phrases.json")
    cleaned_path = Path("table_phrases_cleaned.json")
    backup_path = Path(f"table_phrases_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    report_path = Path("DEDUPLICATION_REPORT.md")

    results = find_and_remove_duplicates(input_path, cleaned_path)

    shutil.copy2(input_path, backup_path)
    print(f"üíæ –°–æ–∑–¥–∞–Ω backup –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {backup_path}")

    shutil.copy2(cleaned_path, input_path)
    print(f"‚úÖ –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª {input_path.name} –∑–∞–º–µ–Ω–µ–Ω –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é")

    generate_markdown_report(
        results,
        report_path,
        main_file=input_path,
        cleaned_file=cleaned_path,
        backup_file=backup_path,
    )
    print(f"üìù –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_path}")

    print("\n" + "=" * 80)
    print("‚úÖ –û–ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print("=" * 80)


if __name__ == "__main__":
    main()
